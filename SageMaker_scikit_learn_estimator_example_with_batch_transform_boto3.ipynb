{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Training and Prediction with Sagemaker Scikit-learn\n",
    "This tutorial shows you how to use [Scikit-learn](https://scikit-learn.org/stable/) with SageMaker by utilizing the pre-built container. Scikit-learn is a popular Python machine learning framework. It includes a number of different algorithms for classification, regression, clustering, dimensionality reduction, and data/feature pre-processing. \n",
    "\n",
    "The [SageMaker boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html) is a low-level client representing Amazon SageMaker Service. It provides APIs for creating and managing SageMaker resources. We will show how to train a model on the Iris dataset and generating a set of predictions. For more information about the Scikit-learn container, see the [sagemaker-scikit-learn-containers](https://github.com/aws/sagemaker-scikit-learn-container) repository and the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) repository.\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook takes approximately 15 minutes to run.\n",
    "\n",
    "## Contents\n",
    "* [Upload the data for training](#upload_data)\n",
    "* [Create a Scikit-learn script to train with](#create_sklearn_script)\n",
    "* [Create the SageMaker Scikit Training Job](#create_sklearn_estimator)\n",
    "* [Train the SKLearn Estimator on the Iris data](#train_sklearn)\n",
    "* [Use the trained model to make inference requests](#inference)\n",
    " * [Deploy the model](#deploy)\n",
    " * [Choose some data and use it for a prediction](#prediction_request)\n",
    " * [Endpoint cleanup](#endpoint_cleanup)\n",
    "* [Batch Transform](#batch_transform)\n",
    " * [Prepare Input Data](#prepare_input_data)\n",
    " * [Run Transform Job](#run_transform_job)\n",
    " * [Check Output Data](#check_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U sagemaker\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import utils\n",
    "import re\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.s3 import S3Uploader, s3_path_join\n",
    "\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# S3 prefix\n",
    "prefix = \"sagemaker/DEMO-scikit-iris\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.client('sagemaker')\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = sagemaker_session.boto_region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "When training large models with huge amounts of data, you may use big data tools like Amazon Athena, AWS Glue, or Amazon EMR to process your data backed by S3. For the purposes of this example, we're using a sample of the classic [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris). We load the dataset, write it locally, then upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.download_file(\n",
    "    f\"sagemaker-sample-files\", \"datasets/tabular/iris/iris.data\", \"./data/iris.csv\"\n",
    ")\n",
    "\n",
    "df_iris = pd.read_csv(\"./data/iris.csv\", header=None)\n",
    "df_iris[4] = df_iris[4].map({\"Iris-setosa\": 0, \"Iris-versicolor\": 1, \"Iris-virginica\": 2})\n",
    "iris = df_iris[[4, 0, 1, 2, 3]].to_numpy()\n",
    "np.savetxt(\"./data/iris.csv\", iris, delimiter=\",\", fmt=\"%1.1f, %1.3f, %1.3f, %1.3f, %1.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data locally, we can use use the tools provided by the SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = \"data\"\n",
    "\n",
    "train_input = sagemaker_session.upload_data(\n",
    "    WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Scikit-learn script for training <a class=\"anchor\" id=\"create_sklearn_script\"></a>\n",
    "SageMaker can run a scikit-learn script using the `SKLearn` estimator. When run on SageMaker, a number of helpful environment variables are available to access properties of the training environment, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "* `SM_OUTPUT_DIR`: A string representing the file system path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "Supposing two input channels, 'train' and 'test', were used in the call to the `SKLearn` estimator's `fit()` method, the following environment variables are set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAIN`: A string representing the path to the directory containing data in the 'train' channel.\n",
    "* `SM_CHANNEL_TEST`: Same as above, but for the 'test' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to the `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. For example, the script that we run in this notebook is below:\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--max_leaf_nodes', type=int, default=-1)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "\n",
    "    # Here we support a single hyperparameter, 'max_leaf_nodes'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    max_leaf_nodes = args.max_leaf_nodes\n",
    "\n",
    "    # Now use scikit-learn's decision tree classifier to train the model.\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Scikit-learn container imports your training script, you should always put your training code in a main guard `(if __name__=='__main__':)` so that the container does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For more information about training environment variables, please visit https://github.com/aws/sagemaker-containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SageMaker SKLearn Training Job <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "To run our Scikit-learn training script on SageMaker, we submit a SageMaker training job using boto3 api. [create_training_job()](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_training_job) and provide necessary parameters required by the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = retrieve(framework=\"sklearn\", region=boto3.Session().region_name, version=\"0.23-1\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_file = \"sourcedir.tar.gz\"\n",
    "utils.create_tar(tar_file, Path(\"code\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_job_name = f\"sklearn-demo-boto3-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "\n",
    "sourcedir_path = s3_path_join(\"s3://\", bucket, f\"{prefix}/{sagemaker_job_name}/source_code\")\n",
    "print(f\"Uploading source code to {sourcedir_path}\")\n",
    "sourcedir_uri = S3Uploader.upload(\"sourcedir.tar.gz\", sourcedir_path)\n",
    "print(f\"Uploaded roberta model to {model_roberta_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_training_params = {\n",
    "    \"TrainingJobName\": sagemaker_job_name,\n",
    "    \"AlgorithmSpecification\": {\n",
    "        \"TrainingImage\": image,\n",
    "        \"TrainingInputMode\": \"File\"\n",
    "    },\n",
    "    \"RoleArn\": role,\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": \"s3://sagemaker-us-east-1-631450739534/sagemaker/DEMO-scikit-iris-sdk/data\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\n",
    "        \"S3OutputPath\": \"s3://sagemaker-us-east-1-631450739534/\",\n",
    "    },\n",
    "    \"ResourceConfig\": {\n",
    "        \"InstanceType\": \"ml.m5.xlarge\",\n",
    "        \"InstanceCount\": 1,\n",
    "        \"VolumeSizeInGB\": 10\n",
    "    },\n",
    "    \"StoppingCondition\": {\n",
    "        \"MaxRuntimeInSeconds\": 3600,\n",
    "        'MaxWaitTimeInSeconds': 3600\n",
    "    },\n",
    "    \"EnableManagedSpotTraining\": True,\n",
    "    \"Environment\": {\n",
    "        \"USE_SMDEBUG\": \"0\",\n",
    "    },\n",
    "    \"HyperParameters\": {\n",
    "        \"max_leaf_nodes\": \"30\",\n",
    "        \"sagemaker_program\": \"scikit_learn_iris.py\",\n",
    "        \"sagemaker_submit_directory\": sourcedir_uri,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a sklearn model on Iris data <a class=\"anchor\" id=\"train_sklearn\"></a>\n",
    "Training is straightforward, just create a training job based on the parameters prepared above! This starts a SageMaker training job that downloads the data, invokes our scikit-learn code (in the provided script file), and saves any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.create_training_job(**create_training_params)\n",
    "status = sm_client.describe_training_job(TrainingJobName=sagemaker_job_name)[\"TrainingJobStatus\"]\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sm_client.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=sagemaker_job_name)\n",
    "finally:\n",
    "    status = sm_client.describe_training_job(TrainingJobName=sagemaker_job_name)[\"TrainingJobStatus\"]\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = sm_client.describe_training_job(TrainingJobName=sagemaker_job_name)[\"FailureReason\"]\n",
    "        print(\"Training failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Training job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the trained model to make inference requests <a class=\"anchor\" id=\"inference\"></a>\n",
    "\n",
    "### Let's create a model based on our training job <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "The below cell creates a model in SageMaker based on the training job we just executed. The model can later be deployed using the SageMaker hosting services or in our case used in a Batch Transform job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = sagemaker_job_name\n",
    "print(model_name)\n",
    "\n",
    "info = sm_client.describe_training_job(TrainingJobName=sagemaker_job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "primary_container = {\n",
    "    \"Image\": image,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "    \"Environment\": {\n",
    "        \"SAGEMAKER_CONTAINER_LOG_LEVEL\": \"20\",\n",
    "        \"SAGEMAKER_REGION\": region,\n",
    "        \"SAGEMAKER_SUBMIT_DIRECTORY\": sourcedir_uri,\n",
    "        \"SAGEMAKER_PROGRAM\": \"scikit_learn_iris.py\"\n",
    "    },\n",
    "}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer=primary_container,\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>\n",
    "In SageMaker Batch Transform, we introduced a new attribute called DataProcessing.In the below cell, we use the [Boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_transform_job) to kick-off several Batch Transform jobs using different configurations of DataProcessing. Please refer to [Associate Prediction Results with Input Records](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to utilize the **DataProcessing** attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare Input Data <a class=\"anchor\" id=\"prepare_input_data\"></a>\n",
    "We extract 10 random samples of 100 rows from the training data, split the features (X) from the labels (Y), and upload the input data to a given location in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Randomly sample the iris dataset 10 times, then split X and Y\n",
    "mkdir -p batch_data/XY batch_data/X batch_data/Y\n",
    "for i in {0..9}; do\n",
    "    cat data/iris.csv | shuf -n 100 > batch_data/XY/iris_sample_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f2- > batch_data/X/iris_sample_X_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f1 > batch_data/Y/iris_sample_Y_${i}.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input data from local file system to S3\n",
    "batch_input_s3_noID = sagemaker_session.upload_data(\"batch_data/X\", key_prefix=f\"{prefix}/batch_input_noID\")\n",
    "batch_input_s3_withID = sagemaker_session.upload_data(\"batch_data/XY\", key_prefix=f\"{prefix}/batch_input_withID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Without data processing\n",
    "Let's first set the data processing fields to null and inspect the inference results. We'll use it as a baseline to compare to the results with data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_name_noID = f\"Batch-Transform-noID-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "input_location = batch_input_s3_noID # use input data without ID column\n",
    "output_location = f\"s3://{bucket}/{prefix}/output/{batch_job_name_noID}\"\n",
    "\n",
    "request = {\n",
    "    \"TransformJobName\": batch_job_name_noID,\n",
    "    \"ModelName\": sagemaker_job_name,\n",
    "    \"TransformOutput\": {\n",
    "        \"S3OutputPath\": output_location,\n",
    "        \"Accept\": \"text/csv\",\n",
    "        \"AssembleWith\": \"Line\",\n",
    "    },\n",
    "    \"TransformInput\": {\n",
    "        \"DataSource\": {\"S3DataSource\": {\"S3DataType\": \"S3Prefix\", \"S3Uri\": input_location}},\n",
    "        \"ContentType\": \"text/csv\",\n",
    "        \"SplitType\": \"Line\",\n",
    "        \"CompressionType\": \"None\",\n",
    "    },\n",
    "    \"TransformResources\": {\"InstanceType\": \"ml.m4.xlarge\", \"InstanceCount\": 1},\n",
    "}\n",
    "\n",
    "sm_client.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name_noID)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sm_client.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name_noID)\n",
    "finally:\n",
    "    response = sm_client.describe_transform_job(TransformJobName=batch_job_name_noID)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(f\"Transform job ended with status: {status}\")\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(f\"Transform failed with the following error: {message}\")\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. It should show the list probabilities of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Output Data  <a class=\"anchor\" id=\"check_output_data\"></a>\n",
    "After the transform job has completed, download the output data from S3. For each file \"f\" in the input data, we have a corresponding file \"f.out\" containing the predicted labels from each input row. We can compare the predicted labels to the true labels saved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local file system\n",
    "response = sm_client.describe_transform_job(TransformJobName=batch_job_name_noID)\n",
    "batch_output = response[\"TransformOutput\"][\"S3OutputPath\"]\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# For each sample file, compare the predicted labels from batch output to the true labels\n",
    "for i in {1..9}; do\n",
    "    diff -s batch_data/Y/iris_sample_Y_${i}.csv \\\n",
    "        <(cat batch_data/output/iris_sample_X_${i}.csv.out | sed 's/[[\"]//g' | sed 's/, \\|]/\\n/g') \\\n",
    "        | sed \"s/\\/dev\\/fd\\/63/batch_data\\/output\\/iris_sample_X_${i}.csv.out/\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Join the input and the prediction results\n",
    "Now, let's use the new feature to associate the prediction results with their corresponding input records. We can also use the InputFilter to exclude the ID column easily and there's no need to have a separate file in S3.\n",
    "\n",
    "- Set InputFilter to \"\\$[1:]\": indicates that we are excluding column 0 (the 'ID') before processing the inferences and keeping everything from column 1 to the last column (all the features or predictors)\n",
    "- Set JoinSource to \"Input\": indicates our desire to join the input data with the inference results\n",
    "- Leave OutputFilter to default (\"$\"), indicating that the joined input and inference results be will saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_job_name_ID = f\"Batch-Transform-withID-{datetime.datetime.now():%Y-%m-%d-%H-%M-%S}\"\n",
    "input_location = batch_input_s3_withID  # use input data with ID column cause InputFilter will filter it out\n",
    "output_location = f\"s3://{bucket}/{prefix}/output/{batch_job_name_ID}\"\n",
    "\n",
    "request[\"TransformJobName\"] = batch_job_name_ID\n",
    "request[\"TransformInput\"][\"DataSource\"][\"S3DataSource\"][\"S3Uri\"] = input_location\n",
    "request[\"TransformOutput\"][\"S3OutputPath\"] = output_location\n",
    "\n",
    "request[\"DataProcessing\"] = {\n",
    "    \"InputFilter\": \"$[1:]\",  # exclude the ID column (index 0)\n",
    "    \"JoinSource\": \"Input\",  # join the input with the inference results\n",
    "}\n",
    "\n",
    "sm_client.create_transform_job(**request)\n",
    "print(\"Created Transform job with name: \", batch_job_name_ID)\n",
    "\n",
    "# Wait until the job finishes\n",
    "try:\n",
    "    sm_client.get_waiter(\"transform_job_completed_or_stopped\").wait(TransformJobName=batch_job_name_ID)\n",
    "finally:\n",
    "    response = sm_client.describe_transform_job(TransformJobName=batch_job_name_ID)\n",
    "    status = response[\"TransformJobStatus\"]\n",
    "    print(\"Transform job ended with status: \" + status)\n",
    "    if status == \"Failed\":\n",
    "        message = response[\"FailureReason\"]\n",
    "        print(\"Transform failed with the following error: {}\".format(message))\n",
    "        raise Exception(\"Transform job failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local file system\n",
    "response = sm_client.describe_transform_job(TransformJobName=batch_job_name_ID)\n",
    "batch_output = response[\"TransformOutput\"][\"S3OutputPath\"]\n",
    "!mkdir -p batch_data/output_ID\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output_ID/\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output_ID/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_list: list):\n",
    "    # Concat input files with select columns\n",
    "    dfs = []\n",
    "    for file in file_list:\n",
    "        df = pd.read_csv(file, header=None)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True, axis=0)\n",
    "\n",
    "output_dir = 'batch_data/output_ID'\n",
    "output_file_list = glob.glob(f\"{output_dir}/*.out\")\n",
    "data_df = load_data(output_file_list)\n",
    "\n",
    "assert data_df.iloc[:,0].values.tolist() == data_df.iloc[:,-1].values.tolist(), \\\n",
    "        \"outputs are different from input\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
